{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    " <b>HOMEWORK</b>\n",
    "</div>\n",
    "In this homework, we will use the Bank Marketing dataset.\n",
    "wget https://archive.ics.uci.edu/static/public/222/bank+marketing.zip\n",
    "We need to take bank/bank-full.csv file from the downloaded zip-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45211\n",
      "Missing values in features:\n",
      "age          0\n",
      "job          0\n",
      "marital      0\n",
      "education    0\n",
      "balance      0\n",
      "housing      0\n",
      "contact      0\n",
      "day          0\n",
      "month        0\n",
      "duration     0\n",
      "campaign     0\n",
      "pdays        0\n",
      "previous     0\n",
      "poutcome     0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Missing values in target:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_csv('bank-full.csv', delimiter=';')\n",
    "\n",
    "# In this dataset our desired target for classification task will be y variable - has the client subscribed a term deposit or not.\n",
    "# For the rest of the homework, you'll need to use only these columns:\n",
    "# age,job,marital,education,balance,housing,contact,day,month,duration,campaign,pdays,previous,poutcome,y\n",
    "\n",
    "# 1. Select only the features from above.\n",
    "features = [\n",
    "    'age',\n",
    "    'job',\n",
    "    'marital',\n",
    "    'education',\n",
    "    'balance',\n",
    "    'housing',\n",
    "    'contact',\n",
    "    'day',\n",
    "    'month',\n",
    "    'duration',\n",
    "    'campaign',\n",
    "    'pdays',\n",
    "    'previous',\n",
    "    'poutcome',\n",
    "]\n",
    "\n",
    "df_features = df_full[features]\n",
    "df_target = df_full['y']\n",
    "\n",
    "df = pd.concat([df_features, df_target], axis=1)\n",
    "print(len(df))\n",
    "\n",
    "# 2. Check if the missing values are present in the dataset.\n",
    "missing_values_features = df_features.isnull().sum()\n",
    "print(f'Missing values in features:\\n{missing_values_features}')\n",
    "print('\\n')\n",
    "missing_value_target = df_target.isnull().sum()\n",
    "print(f'Missing values in target:\\n{missing_value_target}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The mode for education is: secondary\n"
     ]
    }
   ],
   "source": [
    "## Question 1\n",
    "# What is the most frequent observation (mode) for the column education?\n",
    "\n",
    "mode_education = df_features['education'].mode()[0]\n",
    "print(f' The mode for education is: {mode_education}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two features with the highest correlation are: ('previous', 'pdays') with a correlation of 0.4548196354805043\n"
     ]
    }
   ],
   "source": [
    "## Question 2\n",
    "# Create the correlation matrix for the numerical features of your dataset. In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "# What are the two features that have the biggest correlation?\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_df_features = df_features.select_dtypes(include=[float, int])\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numeric_df_features.corr()\n",
    "# Check the two features with the biggest correlation\n",
    "# Unstack the correlation matrix to get a Series\n",
    "correlation_series = correlation_matrix.unstack()\n",
    "# Remove self-correlation (correlation of a feature with itself)\n",
    "correlation_series = correlation_series[correlation_series.index.get_level_values(0) != correlation_series.index.get_level_values(1)]\n",
    "# Sort the Series by the absolute value of the correlation coefficients\n",
    "sorted_correlation_series = correlation_series.abs().sort_values(ascending=False)\n",
    "\n",
    "# Get the pair of features with the highest correlation\n",
    "highest_correlation_pair = sorted_correlation_series.idxmax()\n",
    "highest_correlation_value = sorted_correlation_series.max()\n",
    "print(f\"The two features with the highest correlation are: {highest_correlation_pair} with a correlation of {highest_correlation_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target encoding\n",
    "# Now we want to encode the y variable.\n",
    "# Let's replace the values yes/no with 1/0.\n",
    "\n",
    "df_target = df_target.replace({'yes': 1, 'no': 0})\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45211\n",
      "45211\n",
      "27126 9042 9043\n"
     ]
    }
   ],
   "source": [
    "# Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
    "# Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n",
    "# Make sure that the target value y is not in your dataframe.\n",
    "\n",
    "X_train_val, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "print(len(df_features))\n",
    "print(len(X_train) + len(X_val) + len(X_test))\n",
    "print(len(X_train), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Variable with biggest mutual information score is:\n",
      "\n",
      "poutcome with a score of 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriel.rangel\\AppData\\Local\\Temp\\ipykernel_17084\\1745777648.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Y_train = X_train['y'].replace({'yes': 1, 'no': 0})\n",
      "C:\\Users\\gabriel.rangel\\AppData\\Local\\Temp\\ipykernel_17084\\1745777648.py:29: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f' The Variable with biggest mutual information score is:\\n\\n{highest_mi_score[0]} with a score of {highest_mi_score[1]}')\n"
     ]
    }
   ],
   "source": [
    "## Question 3\n",
    "# Calculate the mutual information score between y and other categorical variables in the dataset. Use the training set only.\n",
    "# Round the scores to 2 decimals using round(score, 2).\n",
    "# Which of the following features has the highest mutual information score with the target variable y?\n",
    "\n",
    "# Select only categorical columns\n",
    "X_train_categorical = X_train.select_dtypes(include=[object])\n",
    "Y_train = X_train['y'].replace({'yes': 1, 'no': 0})\n",
    "X_train_categorical = X_train_categorical.drop(columns='y')\n",
    "\n",
    "\n",
    "# Select only categorical columns names in a list\n",
    "#categorical_columns = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Calculate the mutual information score between y and each categorical feature\n",
    "mutual_info_scores = []\n",
    "for column in X_train_categorical.columns:\n",
    "    score = mutual_info_score(X_train_categorical[column], Y_train)\n",
    "    mutual_info_scores.append((column, score))\n",
    "\n",
    "# Convert the list of tuples to a DataFrame for sorting and rounding\n",
    "mi_scores_df = pd.DataFrame(mutual_info_scores, columns=['Feature', 'MI Score'])\n",
    "\n",
    "# Sort by highest mutual information score and round to 2 decimals\n",
    "mi_scores_df = mi_scores_df.sort_values(by='MI Score', ascending=False).round(2)\n",
    "highest_mi_score = mi_scores_df.iloc[0]\n",
    "\n",
    "# Display the feature with the highest mutual information score\n",
    "print(f' The Variable with biggest mutual information score is:\\n\\n{highest_mi_score[0]} with a score of {highest_mi_score[1]}')\n",
    "\n",
    "# one hot encoding with feature-engine (my preferred method)\n",
    "# from feature_engine import encoding\n",
    "# onehot = encoding.OneHotEncoder(variables=categorical_columns)\n",
    "# onehot.fit(X_train_categorical)\n",
    "# X_train_categorical_feature = onehot.transform(X_train_categorical)\n",
    "# X_train_categorical_feature\n",
    "\n",
    "# one hot encoding with DictVectorizer method\n",
    "# dv = DictVectorizer(sparse=False)\n",
    "# X_train_categorical_dict = X_train_categorical.to_dict(orient='records')\n",
    "# X_train_categorical_encoded = dv.fit_transform(X_train_categorical_dict)\n",
    "# X_train_categorical_encoded = pd.DataFrame(X_train_categorical_encoded, columns=dv.get_feature_names_out())\n",
    "# X_train_categorical_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabriel.rangel\\AppData\\Local\\Temp\\ipykernel_17084\\156047848.py:30: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  Y_val = X_val['y'].replace({'yes': 1, 'no': 0})\n"
     ]
    }
   ],
   "source": [
    "## Question 4\n",
    "# Now let's train a logistic regression.\n",
    "# Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
    "# Fit the model on the training dataset.\n",
    "# To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "# model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "# Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
    "\n",
    "x_train = X_train.drop(columns='y')\n",
    "x_val = X_val.drop(columns='y')\n",
    "\n",
    "# one hot encoding with DictVectorizer method\n",
    "X_train_dict = x_train.to_dict(orient='records')\n",
    "X_val_dict = x_val.to_dict(orient='records')\n",
    "#X_test_dict = X_test.to_dict(orient='records')\n",
    "\n",
    "# Initialize the DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Apply DictVectorizer to the entire dataset (both numerical and categorical features)\n",
    "X_train_encoded = dv.fit_transform(X_train_dict)\n",
    "X_val_encoded = dv.transform(X_val_dict)\n",
    "#X_test_encoded = dv.transform(X_test_dict)\n",
    "\n",
    "# Train the logistic regression model with the specified parameters\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_encoded, Y_train) # Y_train is the target variable that we set on previous exercise\n",
    "\n",
    "# # We need the Y_val to calculate the accuracy\n",
    "Y_val = X_val['y'].replace({'yes': 1, 'no': 0})\n",
    "\n",
    "# Calculate the accuracy on the validation dataset and round it to 2 decimal digits\n",
    "# (Option 1)\n",
    "original_accuracy = model.score(X_val_encoded,Y_val)\n",
    "accuracy_rounded = round(original_accuracy, 2)\n",
    "\n",
    "\n",
    "# Option 2 - using the predict method and accuracy_score\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# # Make predictions on the validation set\n",
    "# y_pred = model.predict(X_val_encoded)\n",
    "\n",
    "# # Calculate accuracy and round to 2 decimal places\n",
    "# original_accuracy = accuracy_score(Y_val, y_pred)\n",
    "# accuracy_rounded = round(original_accuracy, 2)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_rounded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Differences:\n",
      "('age', 0.0003)\n",
      "('balance', 0.0002)\n",
      "('campaign', 0.0007)\n",
      "('contact=cellular', 0.0007)\n",
      "('contact=telephone', 0.0009)\n",
      "('contact=unknown', 0.0007)\n",
      "('day', 0.0006)\n",
      "('duration', 0.0129)\n",
      "('education=primary', 0.0009)\n",
      "('education=secondary', 0.0003)\n",
      "('education=tertiary', 0.0002)\n",
      "('education=unknown', 0.001)\n",
      "('housing=no', 0.0007)\n",
      "('housing=yes', 0.0006)\n",
      "('job=admin.', 0.0002)\n",
      "('job=blue-collar', 0.0008)\n",
      "('job=entrepreneur', 0.0006)\n",
      "('job=housemaid', 0.0006)\n",
      "('job=management', 0.0003)\n",
      "('job=retired', 0.0004)\n",
      "('job=self-employed', 0.0008)\n",
      "('job=services', 0.0006)\n",
      "('job=student', 0.0001)\n",
      "('job=technician', 0.0011)\n",
      "('job=unemployed', 0.0001)\n",
      "('job=unknown', 0.0007)\n",
      "('marital=divorced', 0.0006)\n",
      "('marital=married', 0.0004)\n",
      "('marital=single', 0.0009)\n",
      "('month=apr', 0.0006)\n",
      "('month=aug', 0.0006)\n",
      "('month=dec', 0.0008)\n",
      "('month=feb', 0.0011)\n",
      "('month=jan', 0.0004)\n",
      "('month=jul', 0.0)\n",
      "('month=jun', 0.0002)\n",
      "('month=mar', 0.0002)\n",
      "('month=may', 0.0001)\n",
      "('month=nov', 0.0006)\n",
      "('month=oct', 0.0011)\n",
      "('month=sep', 0.0013)\n",
      "('pdays', 0.0006)\n",
      "('poutcome=failure', 0.0004)\n",
      "('poutcome=other', 0.0008)\n",
      "('poutcome=success', 0.0011)\n",
      "('poutcome=unknown', 0.0009)\n",
      "('previous', 0.0003)\n",
      "\n",
      "Feature with the smallest impact: month=jul (Difference: 0.0)\n",
      "\n",
      "Among the specified features, the one with the smallest impact is: balance (Difference: 0.0002)\n"
     ]
    }
   ],
   "source": [
    "## Question 5\n",
    "# Let's find the least useful feature using the feature elimination technique.\n",
    "# Train a model with all these features (using the same parameters as in Q4).\n",
    "# Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "# For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
    "# Which of following feature has the smallest difference? (age,balance,marital or previous)\n",
    "\n",
    "# Store the differences in accuracy\n",
    "accuracy_differences = []\n",
    "\n",
    "# Iterate over the features and remove one at a time\n",
    "for feature in dv.get_feature_names_out():\n",
    "    # Get the index of the feature to exclude\n",
    "    feature_index = [i for i, f in enumerate(dv.get_feature_names_out()) if f == feature]\n",
    "\n",
    "    # Create new datasets without the specified feature\n",
    "    X_train_reduced = np.delete(X_train_encoded, feature_index, axis=1)\n",
    "    X_val_reduced = np.delete(X_val_encoded, feature_index, axis=1)\n",
    "\n",
    "    # Train the model without the feature\n",
    "    model.fit(X_train_reduced, Y_train)\n",
    "\n",
    "    accuracy = model.score(X_val_reduced,Y_val)\n",
    "\n",
    "\n",
    "    # Calculate the absolute difference in accuracy\n",
    "    accuracy_diff = abs(original_accuracy - accuracy)\n",
    "    accuracy_differences.append((feature, round(accuracy_diff, 4)))\n",
    "\n",
    "# Find the feature with the smallest difference\n",
    "smallest_diff_feature = min(accuracy_differences, key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nAccuracy Differences:\")\n",
    "for diff in accuracy_differences:\n",
    "    print(diff)\n",
    "\n",
    "print(f\"\\nFeature with the smallest impact: {smallest_diff_feature[0]} (Difference: {smallest_diff_feature[1]})\")\n",
    "\n",
    "# Filter the results to compare only 'age', 'balance', 'marital', 'previous'\n",
    "specified_features = ('age', 'balance', 'marital', 'previous')\n",
    "filtered_differences = [item for item in accuracy_differences if item[0].startswith(specified_features)]\n",
    "\n",
    "# Find the specified feature with the smallest difference\n",
    "smallest_diff_specified = min(filtered_differences, key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nAmong the specified features, the one with the smallest impact is: {smallest_diff_specified[0]} (Difference: {smallest_diff_specified[1]})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01, Validation Accuracy: 0.898\n",
      "C = 0.1, Validation Accuracy: 0.901\n",
      "C = 1, Validation Accuracy: 0.902\n",
      "C = 10, Validation Accuracy: 0.901\n",
      "C = 100, Validation Accuracy: 0.901\n",
      "\n",
      "The best C value is: 1 with a validation accuracy of 0.902\n"
     ]
    }
   ],
   "source": [
    "# # Question 6\n",
    "# Now let's train a regularized logistic regression.\n",
    "# Let's try the following values of the parameter C: [0.01, 0.1, 1, 10, 100].\n",
    "# Train models using all the features as in Q4.\n",
    "# Calculate the accuracy on the validation dataset and round it to 3 decimal digits.\n",
    "# Which of these C leads to the best accuracy on the validation set?\n",
    "\n",
    "# Define the values of the parameter C to try\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Store the accuracies for each value of C\n",
    "accuracy_results = []\n",
    "\n",
    "for C in C_values:\n",
    "    # Train the logistic regression model with the specified parameter C\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_encoded, Y_train)\n",
    "\n",
    "    # Calculate the accuracy on the validation dataset\n",
    "    accuracy = model.score(X_val_encoded,Y_val)\n",
    "    accuracy_results.append((C, round(accuracy, 3)))\n",
    "\n",
    "    print(f\"C = {C}, Validation Accuracy: {round(accuracy, 3)}\")\n",
    "\n",
    "# Find the C value that leads to the best accuracy on the validation set\n",
    "best_C, best_accuracy = max(accuracy_results, key=lambda x: x[1])\n",
    "print(f\"\\nThe best C value is: {best_C} with a validation accuracy of {best_accuracy}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
